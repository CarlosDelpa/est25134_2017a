---
title: "S01 - Estimación de curvas (unidimensional)"
author: "Juan Carlos Martínez-Ovando"
date: "Primavera 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
```

## Datos

Iniciamos la sesión importando los datos `Curve_Data.Rdata` del repositorio del curso.

```{r curve_data, echo=TRUE}
rm(list=ls())
githubURL <- "https://github.com/jcmartinezovando/est25134_2017a/raw/master/datos/Curve_Data.RData"

# For Windows
load(url(githubURL))

# If trouble, try this on Linux or iOS
download.file(githubURL,"Curve_Data")
load("Curve_Data")
ls()

# Libraries
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(MASS)
library(RColorBrewer)
```

Pensemos que deseamos descrifrar el patrón de la relación que subyace a los `datos` representados en la siguiente gráfica (considerando $y$ como variable de respuesta).

```{r curve_plot, echo=TRUE}
plot(datos, pch=19, cex=.4, col="blue")
summary(datos)
```

## Contexto

En esta sesión revisaremos los conceptos fundamentales para la estimación de curvas de modelos relacionales. Iniciaremos con la estimación de curvas de respuesta en una dimensión.

El modelo que estaremos explorando es el que empezamos a estudiar en la sesión anterior, i.e. $Y$ es la variable de respuesta (con soporte en $\Re$), y $X$ es un conjunto de covariables (tomando valores en $\Re^p$).

Así, La relación de $Y$ en respuesta de $X$ es
$$
Y|X \sim N(y|f(X),\sigma^2)
$$

En este caso, la esperanza de $Y$ condicional en $X$ es,
$$
\mathbb{E}(y|X)=f(X),
$$
suponiendo que $f(\cdot)$ tiene una forma structural flexible. En particular, trabajaremos en el ejemplo de esta sesion con $f$ miembro de la clase de funciones polinomiales, con
$$
f(x)=\sum_{j=0}^{\infty}\alpha_j \phi_j(x),
$$
donde 
$$
\phi_j(x)=x^{j},
$$
con $x\in (0,1)$.

Los `datos` $(y,x)$ corresponden a la **muestra de entrenamiento**. Iniciamos realizando una inspección descriptiva de los datos (primeros 100), contrastada con $f$ ***(aunque, en aplicaciones reales no la conocemos)***.

```{r curve_descripcion, echo=FALSE}
library(ggplot2)
x_plot <- seq(0,1,0.01)
y_plot <- f(x_plot)
ggplot(datos[c(1:100),], aes(x=x, y=y), colour='red')+
  geom_point() +
  annotate("line", x=x_plot, y=y_plot, linetype="dotted")
```

## Especificación del modelo

Aunque consideramos que $f$ forma parte de la clase de la clase de todos los posibles polinomios en $x$, en la práctica consideraremos generalmente una versión simplificada de su expansión (en este caso, restringiendo el orden de los polinomios a lo más igual a $J$), i.e.
$$
f(x)=\sum_{j=0}^{J}\alpha_j \phi_j(x).
$$
En nuestra aplicación, $J$ es fijo y conicido; más adelante en el curso exploraremos el caso donde $J$ sea aleatorio (a.k.a. desconocido).

Así, los parámetros del modelo son ahora,
$$
\boldsymbol{\alpha}=(\alpha_0,\ldots,\alpha_J) \ \ \text{y} \ \ \sigma^2.
$$

## Aprendizaje

El aprendizaje en este modelo lo realizaremos empleando **máxima verosimilitud**, cuyos estimadores coinciden con los obtenidos por **mínimos cuadrados**, i.e.

```{r}
f_mle <- function(datos, J){
  lm(y ~ poly(x, degree=J, raw = TRUE), 
     data = datos)
  }
```

De esta forma, los estimadores puntuales para $\alpha$ son


Las $\hat{\alpha_j}$ están dadas por:

```{r}
f_alpha <- f_mle(datos, J=6)
data.frame(coef = coef(f_alpha))
```

Notemos que en este caso los datos de entrenamiento corresponden con la muestra completa. Contrastando con la verdadera función $f$ (empleando una submuestra de los primeros 100 datos para efectos de graficación) tenemos,

```{r}
dat <- data.frame(
        x = x_plot, 
        prediccion = predict(f_alpha, newdata=data.frame(x=x_plot)),
        esperado = y_plot)

data_plot <- dat %>% 
        gather(tipo, valor, prediccion:esperado)
head(data_plot)

ggplot(data_plot, aes(x=x, y=valor, linetype=tipo )) + 
    geom_line() +
    ylim(c(-3,3)) +
    annotate("point",x=datos$x, y=datos$y, colour="green")
```

El error cuadratico del aprendizaje de este modelo se calcula como,
$$
\sum_{i=1}^{n}(y_i-\hat{y}_i)^2,
$$
donde 
$$
\hat{y}_i = \hat{f}(x_i)=\sum_{j=0}^{J}\hat{\alpha}_j\phi(x_i),
$$
con $\phi(x_i)=x_i^j$, para $j=1,\ldots,J$, en nuestro caso. Es decir,
```{r}
y_fit <- predict(f_alpha, newdata = datos)
mean((datos$y - y_fit)^2)
cor(datos$y,y_fit)
k <- 11
colors <- rev(brewer.pal(k, "RdYlBu"))
contour <- kde2d(datos$y, y_fit, n=50)
plot(cbind(datos$y,y_fit), xlab="y", ylab="y_fit", pch=19, cex=.4, col="grey")
contour(contour, drawlabels=FALSE, nlevels=k, col=colors, add=TRUE)
abline(h=mean(y_fit), v=mean(datos$y), lwd=2)
```
